{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a46b9a",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "In this Jupyter notebook I will be looking at missing values, outliers and how I am going to handle these.\n",
    "<br>\n",
    "<br>\n",
    "First of I need to calculate how many missing items there are in each row, however the file is too big to load into memory in a single file. This is where dask comes in; dask is a library that handles splitting up files and loading them into memory one at a time and calculating the results. Using dask I can set a maximum size that is allowed to be loaded into memory at ones, 32MB should be fine and still reasonably fast.\n",
    "<br>\n",
    "Below you can see the results of columns with their missing values total and percentage.\n",
    "\n",
    "| Column                          | Missing Count | Missing Percent|\n",
    "|---------------------------------|---------------|----------------|\n",
    "| Service:RDT-ID                  | 0             | 0.000000       |\n",
    "| Service:Date                    | 0             | 0.000000       |\n",
    "| Service:Type                    | 0             | 0.000000       |\n",
    "| Service:Company                 | 0             | 0.000000       |\n",
    "| Service:Train number            | 0             | 0.000000       |\n",
    "| Service:Completely cancelled    | 0             | 0.000000       |\n",
    "| Service:Partly cancelled        | 0             | 0.000000       |\n",
    "| Service:Maximum delay           | 0             | 0.000000       |\n",
    "| Stop:RDT-ID                     | 0             | 0.000000       |\n",
    "| Stop:Station code               | 182,170       | 0.125885       |\n",
    "| Stop:Station name               | 0             | 0.000000       |\n",
    "| Stop:Arrival time               | 16,127,855    | 11.144837      |\n",
    "| Stop:Arrival delay              | 16,127,855    | 11.144837      |\n",
    "| Stop:Arrival cancelled          | 16,127,855    | 11.144837      |\n",
    "| Stop:Departure time             | 15,859,427    | 10.959345      |\n",
    "| Stop:Departure delay            | 15,859,427    | 10.959345      |\n",
    "| Stop:Departure cancelled        | 15,859,427    | 10.959345      |\n",
    "| Stop:Platform change            | 0             | 0.000000       |\n",
    "| Stop:Planned platform           | 16,970,704    | 11.727271      |\n",
    "| Stop:Actual platform            | 16,970,704    | 11.727271      |\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d73eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\bramm\\AppData\\Local\\Temp\\ipykernel_23756\\1267378841.py:5: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  df = pd.read_csv('NS_Data\\combined_trein_data.csv', nrows=1_000_000)\n",
      "C:\\Users\\bramm\\AppData\\Local\\Temp\\ipykernel_23756\\1267378841.py:6: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  df.to_csv('NS_Data\\combined_trein_data_sample.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('NS_Data\\combined_trein_data.csv', nrows=1_000_000)\n",
    "df.to_csv('NS_Data\\combined_trein_data_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f7081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\bramm\\AppData\\Local\\Temp\\ipykernel_23756\\80343318.py:34: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  summary = missing_value_summary(\"NS_Data\\combined_trein_data_sample.csv\", blocksize=\"64MB\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: pyarrow>=10.0.1 is required for PyArrow backed StringArray.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\backends.py:140\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:731\u001b[39m, in \u001b[36mmake_reader.<locals>.read\u001b[39m\u001b[34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    719\u001b[39m     urlpath,\n\u001b[32m    720\u001b[39m     blocksize=\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    729\u001b[39m     **kwargs,\n\u001b[32m    730\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m        \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[43massume_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:628\u001b[39m, in \u001b[36mread_pandas\u001b[39m\u001b[34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m values = [\n\u001b[32m    621\u001b[39m     [\n\u001b[32m    622\u001b[39m         [convert_legacy_task(k, ts, {}) \u001b[38;5;28;01mfor\u001b[39;00m k, ts \u001b[38;5;129;01min\u001b[39;00m dsk.dask.items()]\n\u001b[32m   (...)\u001b[39m\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m values\n\u001b[32m    626\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext_blocks_to_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecified_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspecified_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:275\u001b[39m, in \u001b[36mtext_blocks_to_pandas\u001b[39m\u001b[34m(reader, block_lists, header, head, kwargs, enforce, specified_dtypes, path, blocksize, urlpath)\u001b[39m\n\u001b[32m    273\u001b[39m     parts.append([paths[i] \u001b[38;5;28;01mif\u001b[39;00m paths \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, is_first[i], is_last[i]])\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_read_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[43m=\u001b[49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mread-csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:5951\u001b[39m, in \u001b[36mfrom_map\u001b[39m\u001b[34m(func, args, meta, divisions, label, enforce_metadata, *iterables, **kwargs)\u001b[39m\n\u001b[32m   5950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pyarrow_strings_enabled():\n\u001b[32m-> \u001b[39m\u001b[32m5951\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mArrowStringConversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\_collections.py:8\u001b[39m, in \u001b[36mnew_collection\u001b[39m\u001b[34m(expr)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m meta = \u001b[43mexpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_meta\u001b[49m\n\u001b[32m      9\u001b[39m expr._name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\functools.py:995\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    996\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\dataframe\\dask_expr\\_expr.py:562\u001b[39m, in \u001b[36mBlockwise._meta\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    561\u001b[39m args = [op._meta \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, Expr) \u001b[38;5;28;01melse\u001b[39;00m op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._args]\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\dataframe\\_pyarrow.py:61\u001b[39m, in \u001b[36m_to_string_dtype\u001b[39m\u001b[34m(df, dtype_check, index_check, string_dtype)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m string_dtype == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     string_dtype = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStringDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Possibly convert DataFrame/Series/Index to `string[pyarrow]`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\string_.py:182\u001b[39m, in \u001b[36mStringDtype.__init__\u001b[39m\u001b[34m(self, storage, na_value)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m storage == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pa_version_under10p1:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    183\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow>=10.0.1 is required for PyArrow backed StringArray.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(na_value, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np.isnan(na_value):\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# when passed a NaN value, always set to np.nan to ensure we use\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# a consistent NaN value (and we can use `dtype.na_value is np.nan`)\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: pyarrow>=10.0.1 is required for PyArrow backed StringArray.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     summary = \u001b[43mmissing_value_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNS_Data\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcombined_trein_data_sample.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m64MB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mmissing_value_summary\u001b[39m\u001b[34m(csv_path, blocksize)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03mCompute missing value counts and percentages for a CSV file using Dask.\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33;03m    pd.DataFrame: DataFrame containing missing value counts and percentages per column.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Load CSV lazily with Dask\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m df = \u001b[43mdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[32m     22\u001b[39m     missing_summary = df.isnull().sum().compute()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bramm\\Personal-Challenge-NS-Treinen\\.venv\\Lib\\site-packages\\dask\\backends.py:151\u001b[39m, in \u001b[36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: pyarrow>=10.0.1 is required for PyArrow backed StringArray."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "\n",
    "def missing_value_summary(csv_path, blocksize=\"64MB\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute missing value counts and percentages for a CSV file using Dask.\n",
    "\n",
    "    Parameters:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        blocksize (str or int): Block size for Dask to load the CSV in chunks. Default is \"32MB\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing missing value counts and percentages per column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load CSV lazily with Dask\n",
    "    df = dd.read_csv(csv_path, blocksize=blocksize)\n",
    "\n",
    "    with ProgressBar():\n",
    "        missing_summary = df.isnull().sum().compute()\n",
    "        row_count = df.shape[0].compute()\n",
    "\n",
    "    missing_percent = (missing_summary / row_count) * 100\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"missing_count\": missing_summary,\n",
    "        \"missing_percent\": missing_percent\n",
    "    })\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    summary = missing_value_summary(\"NS_Data\\combined_trein_data_sample.csv\", blocksize=\"64MB\")\n",
    "    print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2518409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def replace_missing_with_none(csv_path, column_name, blocksize=\"64MB\", output_path=None):\n",
    "    \"\"\"\n",
    "    Replace missing values (NaN and empty strings) in a specified column with \"NOCODE\".\n",
    "\n",
    "    Parameters:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        column_name (str): Column to replace missing values in.\n",
    "        blocksize (str or int): Block size for Dask to load the CSV in chunks.\n",
    "        output_path (str, optional): Path to save the modified CSV. If None, returns the Dask DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        dask.dataframe.DataFrame or None: Modified DataFrame if output_path is None, otherwise saves to file.\n",
    "    \"\"\"\n",
    "    # Load CSV lazily\n",
    "    df = dd.read_csv(csv_path, blocksize=blocksize)\n",
    "\n",
    "    # Replace empty strings and NaN values with \"NOCODE\"\n",
    "    df[column_name] = df[column_name].replace(\"\", \"NOCODE\").fillna(\"NOCODE\")\n",
    "\n",
    "    if output_path:\n",
    "        with ProgressBar():\n",
    "            df.to_csv(output_path, single_file=True, index=False)\n",
    "        print(f\"Modified dataset saved to {output_path}\")\n",
    "        return None\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def replace_missing_with_value(csv_path, columns, value, blocksize=\"64MB\", output_path=None):\n",
    "    \"\"\"\n",
    "    Replace missing values (NaN and empty strings) in specified columns with a given value.\n",
    "\n",
    "    Parameters:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "        columns (str or list): Column name(s) to replace missing values in.\n",
    "        value: The value to insert for missing values.\n",
    "        blocksize (str or int): Block size for Dask to load the CSV in chunks.\n",
    "        output_path (str, optional): Path to save the modified CSV. If None, returns the Dask DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        dask.dataframe.DataFrame or None: Modified DataFrame if output_path is None, otherwise saves to file.\n",
    "    \"\"\"\n",
    "    # Ensure columns is a list\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "\n",
    "    # Load CSV lazily\n",
    "    df = dd.read_csv(csv_path, blocksize=blocksize)\n",
    "\n",
    "    # Replace missing values in each specified column\n",
    "    for col in columns:\n",
    "        df[col] = df[col].replace(\"\", value).fillna(value)\n",
    "\n",
    "    if output_path:\n",
    "        with ProgressBar():\n",
    "            df.to_csv(output_path, single_file=True, index=False)\n",
    "        print(f\"Modified dataset saved to {output_path}\")\n",
    "        return None\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991b9bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 102.82 ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\bramm\\AppData\\Local\\Temp\\ipykernel_21224\\488088290.py:2: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"NS_Data\\combined_trein_data_sample.csv\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 8.68 ss\n",
      "Modified dataset saved to NS_Data/combined_trein_data_modified.csv\n",
      "[########################################] | 100% Completed | 7.53 ss\n",
      "Modified dataset saved to NS_Data/combined_trein_data_modified.csv\n",
      "[########################################] | 100% Completed | 7.77 ss\n",
      "Modified dataset saved to NS_Data/combined_trein_data_modified.csv\n",
      "[########################################] | 100% Completed | 7.42 ss\n",
      "Modified dataset saved to NS_Data/combined_trein_data_modified.csv\n",
      "[########################################] | 100% Completed | 7.79 ss\n",
      "Modified dataset saved to NS_Data/combined_trein_data_modified.csv\n",
      "[########################################] | 100% Completed | 2.60 ss\n",
      "[########################################] | 100% Completed | 1.48 ss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Service:RDT-ID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service:Date</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service:Type</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service:Company</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service:Train number</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service:Completely cancelled</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service:Partly cancelled</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Service:Maximum delay</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:RDT-ID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Station code</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Station name</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Arrival time</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Arrival delay</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Arrival cancelled</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Departure time</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Departure delay</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Departure cancelled</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Platform change</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Planned platform</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stop:Actual platform</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              missing_count  missing_percent\n",
       "Service:RDT-ID                            0              0.0\n",
       "Service:Date                              0              0.0\n",
       "Service:Type                              0              0.0\n",
       "Service:Company                           0              0.0\n",
       "Service:Train number                      0              0.0\n",
       "Service:Completely cancelled              0              0.0\n",
       "Service:Partly cancelled                  0              0.0\n",
       "Service:Maximum delay                     0              0.0\n",
       "Stop:RDT-ID                               0              0.0\n",
       "Stop:Station code                         0              0.0\n",
       "Stop:Station name                         0              0.0\n",
       "Stop:Arrival time                         0              0.0\n",
       "Stop:Arrival delay                        0              0.0\n",
       "Stop:Arrival cancelled                    0              0.0\n",
       "Stop:Departure time                       0              0.0\n",
       "Stop:Departure delay                      0              0.0\n",
       "Stop:Departure cancelled                  0              0.0\n",
       "Stop:Platform change                      0              0.0\n",
       "Stop:Planned platform                     0              0.0\n",
       "Stop:Actual platform                      0              0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "replace_missing_with_none(\n",
    "        \"NS_Data\\combined_trein_data_sample.csv\",\n",
    "        \"Stop:Station code\",\n",
    "        blocksize=\"32MB\",\n",
    "        output_path=\"NS_Data/combined_trein_data_modified.csv\"\n",
    ")\n",
    "replace_missing_with_value(\n",
    "        \"NS_Data/combined_trein_data_modified.csv\",\n",
    "        [\"Stop:Planned platform\", \"Stop:Actual platform\"],\n",
    "        1,\n",
    "        blocksize=\"32MB\",\n",
    "        output_path=\"NS_Data/combined_trein_data_modified.csv\"\n",
    ")\n",
    "replace_missing_with_value(\n",
    "    \"NS_Data/combined_trein_data_modified.csv\",\n",
    "    [\"Stop:Arrival time\", \"Stop:Departure time\"],\n",
    "    -1,\n",
    "    blocksize=\"32MB\",\n",
    "    output_path=\"NS_Data/combined_trein_data_modified.csv\"\n",
    ")\n",
    "replace_missing_with_value(\n",
    "    \"NS_Data/combined_trein_data_modified.csv\",\n",
    "    [\"Stop:Arrival delay\", \"Stop:Departure delay\"],\n",
    "    0,\n",
    "    blocksize=\"32MB\",\n",
    "    output_path=\"NS_Data/combined_trein_data_modified.csv\"\n",
    ")\n",
    "replace_missing_with_value(\n",
    "    \"NS_Data/combined_trein_data_modified.csv\",\n",
    "    [\"Stop:Arrival cancelled\", \"Stop:Departure cancelled\"],\n",
    "    False,\n",
    "    blocksize=\"32MB\",\n",
    "    output_path=\"NS_Data/combined_trein_data_modified.csv\"\n",
    ")\n",
    "missing_value_summary(\"NS_Data/combined_trein_data_modified.csv\", blocksize=\"64MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
